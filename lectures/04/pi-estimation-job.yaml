apiVersion: v1
kind: ConfigMap
metadata:
  name: pi-estimation-config
data:
  pi-estimation.py: |
    import sys
    from operator import add
    from random import random

    from utils import SPARK_ENV, get_spark_context

    if __name__ == "__main__":
        """
            Usage: pi [partitions]
        """
        spark = get_spark_context(app_name="Pi estimation", config=SPARK_ENV.LOCAL)
        sc = spark.sparkContext

        partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2
        n = 1000000 * partitions

        def f(_: int) -> float:
            x = random() * 2 - 1
            y = random() * 2 - 1
            return 1 if x ** 2 + y ** 2 <= 1 else 0

        count = sc.parallelize(range(1, n + 1), partitions).map(f).reduce(add)
        print("Pi is roughly %f" % (4.0 * count / n))

        spark.stop()

  utils.py: |
    """
    This module contains utility functions for the Spark applications.
    """

    import locale
    import os
    import re
    import subprocess
    from enum import Enum

    from pyspark import SparkConf
    from pyspark.sql import SparkSession

    locale.getdefaultlocale()
    locale.getpreferredencoding()

    FS: str = "hdfs://namenode:9000/"
    # Get the IP address of the host machine.
    SPARK_DRIVER_HOST = (
        subprocess.check_output(["hostname", "-i"]).decode(encoding="utf-8").strip()
    )

    SPARK_DRIVER_HOST = re.sub(rf"\s*127.0.0.1\s*", "", SPARK_DRIVER_HOST)
    os.environ["SPARK_LOCAL_IP"] = SPARK_DRIVER_HOST


    class SPARK_ENV(Enum):
        LOCAL = [
            ("spark.master", "local"),
            ("spark.driver.host", SPARK_DRIVER_HOST),
        ]
        K8S = [
            ("spark.master", "spark://spark-master-svc:7077"),
            ("spark.driver.bindAddress", "0.0.0.0"),
            ("spark.driver.host", SPARK_DRIVER_HOST),
            ("spark.driver.port", "7077"),
            ("spark.submit.deployMode", "client"),
        ]


    def get_spark_context(app_name: str, config: SPARK_ENV) -> SparkSession:
        """Get a Spark context with the given configuration."""
        spark_conf = SparkConf().setAll(config.value).setAppName(app_name)
        return SparkSession.builder.config(conf=spark_conf).getOrCreate()
---
apiVersion: batch/v1
kind: Job
metadata:
  name: spark-pi-estimation
  labels:
    app: spark-pi
spec:
  template:
    spec:
      containers:
        - name: spark-pi-estimation
          image: bitnami/spark:3.5.2-debian-12-r0
          command: [ "sh", "-c" ]
          args:
            - |
              /opt/bitnami/spark/bin/spark-submit --master spark://spark-master-svc:7077 --conf spark.driver.extraJavaOptions=-Duser.home=/root /opt/bitnami/spark/examples/pi-estimation.py 4;
          env:
            - name: SPARK_LOCAL_DIRS
              value: "/tmp/spark"
            - name: SPARK_CONF_DIR
              value: "/opt/bitnami/spark/conf"
            - name: HOME
              value: "/root"
          volumeMounts:
            - name: pi-scripts
              mountPath: /opt/bitnami/spark/examples/pi-estimation.py
              subPath: pi-estimation.py
            - name: pi-scripts
              mountPath: /opt/bitnami/spark/examples/utils.py
              subPath: utils.py
            - name: ivy-cache
              mountPath: /root/.ivy2
          ports:
            - containerPort: 7077
              name: spark
      volumes:
        - name: pi-scripts
          configMap:
            name: pi-estimation-config
        - name: ivy-cache
          emptyDir: { }
      restartPolicy: Never
  backoffLimit: 2